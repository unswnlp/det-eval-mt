{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/LaBSE')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/LaBSE')\n",
    "model.to(device)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_embeds(sentences):\n",
    "    sentences = [str(s) if pd.notnull(s) else \"\" for s in sentences]\n",
    "    \n",
    "    encoded_input = tokenizer(\n",
    "        sentences, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    return sentence_embeddings\n",
    "\n",
    "def get_similarity(embed_1, embed_2):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    return cos(embed_1, embed_2)[0].item()\n",
    "\n",
    "def get_sim_score(data):\n",
    "    cols = data.columns\n",
    "    sim_scores = {lang: [] for lang in cols[1:]}\n",
    "    for i in tqdm(range(len(data))):\n",
    "        try:\n",
    "            embeds = get_embeds(list(data.iloc[i][cols[:]]))\n",
    "            for j, key in enumerate(sim_scores.keys()):\n",
    "                sim_scores[key].append(get_similarity(embeds[[0]], embeds[[j+1]]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error on row {i}: {e}\")\n",
    "            continue\n",
    "    return sim_scores\n",
    "\n",
    "data = pd.read_csv(\"../data/educhat-translation/all_translations.csv\")\n",
    "sim_score = pd.DataFrame(get_sim_score(data)).mean().reset_index()\n",
    "sim_score.columns = [\"language\", \"score\"]\n",
    "\n",
    "sim_score.to_json(\"../results/refer-less.json\", orient=\"records\", indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
